{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaT8RvUSlf5x"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/UniversalDependencies/UD_English-PUD.git\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-sq9c0LUlw5Y"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "#extract words and their tags from train file\n",
    "sentences = []\n",
    "sentences_tags = []\n",
    "tag_words = []\n",
    "tag_tags = []\n",
    "with open(\"UD_English-ParTUT/en_partut-ud-train.conllu\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "        if line[0] == '1' and line[1] == '\\t':\n",
    "            sentences.append(tag_words)\n",
    "            sentences_tags.append(tag_tags)\n",
    "            tag_words = []\n",
    "            tag_tags = []\n",
    "        splitted = line.split('\\t')\n",
    "        if len(splitted) <= 3:\n",
    "            continue\n",
    "        tag_words.append(splitted[1])\n",
    "        tag_tags.append(splitted[3])\n",
    "        \n",
    "del sentences[0]\n",
    "del sentences_tags[0]\n",
    "\n",
    "train_sentences, train_tags = sentences[:int(len(sentences)* 1)], sentences_tags[:int(len(sentences)* 1)]\n",
    "\n",
    "#extract words and their tags from test file\n",
    "sentences = []\n",
    "sentences_tags = []\n",
    "tag_words = []\n",
    "tag_tags = []\n",
    "with open(\"UD_English-ParTUT/en_partut-ud-test.conllu\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "        if line[0] == '1' and line[1] == '\\t':\n",
    "            sentences.append(tag_words)\n",
    "            sentences_tags.append(tag_tags)\n",
    "            tag_words = []\n",
    "            tag_tags = []\n",
    "        splitted = line.split('\\t')\n",
    "        if len(splitted) <= 3:\n",
    "            continue\n",
    "        tag_words.append(splitted[1])\n",
    "        tag_tags.append(splitted[3])\n",
    "        \n",
    "del sentences[0]\n",
    "del sentences_tags[0]\n",
    "\n",
    "test_sentences, test_tags = sentences, sentences_tags\n",
    "\n",
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "        \n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    "\n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding\n",
    "\n",
    "train_sentences_X, train_tags_y, test_tags_y = [], [], []\n",
    " \n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "\n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    "\n",
    "for s in test_tags:\n",
    "    try:\n",
    "        test_tags_y.append([tag2index[t] for t in s])\n",
    "    except KeyError:\n",
    "        print(test_tags.index(s))\n",
    "        \n",
    "test_sentences_X = []\n",
    "\n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "    \n",
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LENGTH)\n",
    "\n",
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)\n",
    "\n",
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "\n",
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences\n",
    "\n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
    " \n",
    "model.summary()\n",
    "\n",
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=50, validation_split=0.2)\n",
    "\n",
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\") \n",
    "\n",
    "print(test_sentences[10:20])\n",
    "print(\"test tags\", test_tags[10:20])\n",
    "print(\"prediction\")\n",
    "predictions = model.predict([test_sentences_X[10:20]])\n",
    "print(logits_to_tokens(predictions, {i: t for t, i in tag2index.items()}))\n",
    "\n",
    "\n",
    "def evaluate(sent_set, tags_set):\n",
    "    individual_score = 0\n",
    "    sentence_score = 0\n",
    "    total_word_count = 0\n",
    "    final_results = []\n",
    "    idx=0\n",
    "    for i in range(len(sent_set)):\n",
    "        idx+=1\n",
    "        if idx%100==0:\n",
    "            print(str(idx) + \" senteces completed.\")\n",
    "        tag_sequence = tags_set[i]\n",
    "        result = model.predict(np.asarray([sent_set[i]]))\n",
    "        pred = logits_to_tokens(result, {i: t for t, i in tag2index.items()})\n",
    "        for i in range(len(pred[0])):\n",
    "            if '-PAD-' in pred[0]:\n",
    "                i +=1\n",
    "                pred[0].remove('-PAD-')\n",
    "        final_results += pred[0]\n",
    "    \n",
    "        if pred[0]==tag_sequence:\n",
    "            sentence_score+=1\n",
    "            individual_score+=len(pred[0])\n",
    "            total_word_count += len(pred[0])\n",
    "        else:\n",
    "            for predicted, actual in zip(pred[0],tag_sequence):\n",
    "                total_word_count+=1\n",
    "                if predicted==actual:\n",
    "                    individual_score+=1\n",
    "\n",
    "\n",
    "    print(\"Accuracy (tokenwise): \",float(individual_score)/total_word_count)\n",
    "    print(\"Accuracy (sentencewise): \",float(sentence_score)/len(sent_set))\n",
    "\n",
    "evaluate(test_sentences_X, test_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTb8NjOFmRlN"
   },
   "outputs": [],
   "source": [
    "# embedding with pre trained Word2vec model\n",
    "\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "\n",
    "#extract words and their tags from file\n",
    "sentences = []\n",
    "sentences_tags = []\n",
    "tag_words = []\n",
    "tag_tags = []\n",
    "with open(\"UD_English-ParTUT/en_partut-ud-train.conllu\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if line[0] == '#':\n",
    "            continue\n",
    "        if line[0] == '1' and line[1] == '\\t':\n",
    "            sentences.append(tag_words)\n",
    "            sentences_tags.append(tag_tags)\n",
    "            tag_words = []\n",
    "            tag_tags = []\n",
    "        splitted = line.split('\\t')\n",
    "        if len(splitted) <= 3:\n",
    "            continue\n",
    "        tag_words.append(splitted[1])\n",
    "        tag_tags.append(splitted[3])\n",
    "        \n",
    "del sentences[0]\n",
    "del sentences_tags[0]\n",
    "\n",
    "# loaded from https://code.google.com/archive/p/word2vec/\n",
    "news_w2v = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "tags = set([])\n",
    "        \n",
    "for ts in sentences_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    "\n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding\n",
    "\n",
    "tags_y = []\n",
    "\n",
    "for s in sentences_tags:\n",
    "    tags_y.append([tag2index[t] for t in s])\n",
    "        \n",
    "tokenizer = Tokenizer(lower=True, oov_token='-OOV-')\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sentences_X = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "MAX_LENGTH = len(max(sentences_X, key=len))\n",
    "sentences_X = pad_sequences(sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "tags_y = pad_sequences(tags_y,maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "tokenizer.word_index['-PAD-'] = 0\n",
    "\n",
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, news_w2v.wv.vector_size))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_vector = news_w2v.wv.get_vector(word)\n",
    "    except:\n",
    "        embedding_vector = None\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "(train_sentences_X, test_sentences_X, train_tags_y, test_tags_y) = train_test_split(sentences_X, tags_y, test_size=0.2)\n",
    "\n",
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)\n",
    "\n",
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "\n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    " \n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(vocab_size,300,weights=[embedding_matrix],input_length=MAX_LENGTH,trainable=False))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy', ignore_class_accuracy(0)])\n",
    " \n",
    "model.summary()\n",
    "\n",
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=50, validation_split=0.2)\n",
    "\n",
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print(f\"{model.metrics_names[1]}: {scores[1] * 100}\") \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HA1_POS_LSTM",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
